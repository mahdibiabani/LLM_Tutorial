{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRuFso38jxFd"
   },
   "source": [
    "# Welcome to Pipelines!\n",
    "\n",
    "The HuggingFace transformers library provides APIs at two different levels.\n",
    "\n",
    "The High Level API for using open-source models for typical inference tasks is called \"pipelines\". It's incredibly easy to use.\n",
    "\n",
    "You create a pipeline using something like:\n",
    "\n",
    "`my_pipeline = pipeline(\"the_task_I_want_to_do\")`\n",
    "\n",
    "Followed by\n",
    "\n",
    "`result = my_pipeline(my_input)`\n",
    "\n",
    "And that's it!\n",
    "\n",
    "See end of this colab for a list of all pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyvdfSSv4Cwo"
   },
   "source": [
    "## A sidenote:\n",
    "\n",
    "You may already know this, but just in case you're not familiar with the word \"inference\" that I use here:\n",
    "\n",
    "When working with Data Science models, you could be carrying out 2 very different activities: **training** and **inference**.\n",
    "\n",
    "### 1. Training  \n",
    "\n",
    "**Training** is when you provide a model with data for it to adapt to get better at a task in the future. It does this by updating its internal settings - the parameters or weights of the model. If you're Training a model that's already had some training, the activity is called \"fine-tuning\".\n",
    "\n",
    "### 2. Inference\n",
    "\n",
    "**Inference** is when you are working with a model that has _already been trained_. You are using that model to produce new outputs on new inputs, taking advantage of everything it learned while it was being trained. Inference is also sometimes referred to as \"Execution\" or \"Running a model\".\n",
    "\n",
    "All of our use of APIs for GPT, Claude and Gemini in the last weeks are examples of **inference**. The \"P\" in GPT stands for \"Pre-trained\", meaning that it has already been trained with data (lots of it!) In week 6 we will try fine-tuning GPT ourselves.\n",
    "  \n",
    "The pipelines API in HuggingFace is only for use for **inference** - running a model that has already been trained. In week 7 we will be training our own model, and we will need to use the more advanced HuggingFace APIs that we look at in the up-coming lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ03dQDl2h0D"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CTm7gpG7qhB7"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90GeDKCG6c1v"
   },
   "source": [
    "# Important Note\n",
    "\n",
    "I didn't mention this in the lecture, but you may need to log in to the HuggingFace hub if you've not done so before.\n",
    "\n",
    "1. If you haven't already done so, create a **free** HuggingFace account at https://huggingface.co and navigate to Settings from the user menu on the top right. Then Create a new API token, giving yourself write permissions.\n",
    "\n",
    "**IMPORTANT** when you create your HuggingFace API key, please be sure to select read and write permissions for your key by clicking on the WRITE tab, otherwise you may get problems later.\n",
    "\n",
    "2. Back here in colab, press the \"key\" icon on the side panel to the left, and add a new secret:  \n",
    "  In the name field put `HF_TOKEN`  \n",
    "  In the value field put your actual token: `hf_...`  \n",
    "  Ensure the notebook access switch is turned ON.\n",
    "\n",
    "3. Execute the cell below to log in. You'll need to do this on each of your colabs. It's a really useful way to manage your secrets without needing to type them into colab. There's also a shortcut to simply overwrite the line below with:  \n",
    "`hf_token = \"hf_....\"`  \n",
    "But this isn't a best practice, as you'd have to be careful not to share the colab. And one of the great things about colabs is that you can share them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E7GoH-tT6-xD"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzurQ1d12mBU"
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
    "result = classifier(\"I'm super excited to be on the way to LLM mastery!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeSJeFAh21Ra"
   },
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
    "result = ner(\"Barack Obama was the 44th president of the United States.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1fnF2yJ3o6O"
   },
   "outputs": [],
   "source": [
    "# Question Answering with Context\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
    "result = question_answerer(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjiiWRj231ME"
   },
   "outputs": [],
   "source": [
    "# Text Summarization\n",
    "\n",
    "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
    "text = \"\"\"The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
    "\"\"\"\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7UMfw324AdO"
   },
   "outputs": [],
   "source": [
    "# Translation\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
    "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGU7ANVaRIkR"
   },
   "outputs": [],
   "source": [
    "# Another translation, showing a model being specified\n",
    "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
    "\n",
    "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
    "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSZR309b4IP8"
   },
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
    "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_vynLSH4YQ7"
   },
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "\n",
    "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
    "result = generator(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgG4kcT_4lO_"
   },
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "\n",
    "image_gen = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "text = \"A class of Data Scientists learning about AI, in the surreal style of Salvador Dali\"\n",
    "image = image_gen(prompt=text).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCPBE0i4pAAO"
   },
   "outputs": [],
   "source": [
    "# Audio Generation\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "speech = synthesiser(\"Hi to an artificial intelligence engineer, on the way to mastery!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
    "Audio(\"speech.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnIE_KfKOb2S"
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdMBtNNp3FwC"
   },
   "source": [
    "# All the available pipelines\n",
    "\n",
    "Here are all the pipelines available from Transformers and Diffusers.\n",
    "\n",
    "With thanks to student Lucky P for suggesting I include this!\n",
    "\n",
    "There's a list pipelines under the Tasks on this page (you have to scroll down a bit, then expand the parameters to see the Tasks):\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "There's also this list of Tasks for Diffusion models instead of Transformers, following the image generation example where I use DiffusionPipeline above.\n",
    "\n",
    "https://huggingface.co/docs/diffusers/en/api/pipelines/overview\n",
    "\n",
    "If you come up with some cool examples of other pipelines, please share them with me! It's wonderful how HuggingFace makes this advanced AI functionality available for inference with such a simple API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   day1-Copy1.ipynb\n",
      "\tmodified:   day2.ipynb\n",
      "\tmodified:   day3.ipynb\n",
      "\tmodified:   day4.ipynb\n",
      "\tmodified:   day5-Copy1.ipynb\n",
      "\tmodified:   day5.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.ipynb_checkpoints/\n",
      "\tGuide to Jupyter.ipynb\n",
      "\tIntermediate Python.ipynb\n",
      "\tcommunity-contributions/\n",
      "\tday1.ipynb\n",
      "\tday2 EXERCISE.ipynb\n",
      "\tdiagnostics.py\n",
      "\tsolutions/\n",
      "\ttroubleshooting.ipynb\n",
      "\tweek1 EXERCISE.ipynb\n",
      "\tweek_3_day_2_pipelines.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'week_3_day_2_pipelines.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add week_3_day_2_pipelines.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin  main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
